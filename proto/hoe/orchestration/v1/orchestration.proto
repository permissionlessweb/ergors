syntax = "proto3";

package cw_ho.orchestration.v1;

import "google/protobuf/timestamp.proto";
import "google/protobuf/struct.proto";
import "hoe/types/v1/common.proto";
import "hoe/network/v1/network.proto";

// Cosmic Orchestration Types
message CosmicTask {
  string id = 1;
  OrchestrateTask task_type = 2;
  CosmicTaskStatus status = 3;
  string prompt = 4;
  optional FractalRequirements fractal_requirements = 5;
  google.protobuf.Timestamp created_at = 6;
  google.protobuf.Timestamp updated_at = 7;
  optional google.protobuf.Struct result = 8;
  optional string error = 9;
}

enum OrchestrateTask {
  ORCHESTRATE_TASK_UNSPECIFIED = 0;
  ORCHESTRATE_TASK_BOOTSTRAP = 1;
  ORCHESTRATE_TASK_RECURSIVE = 2;
}

enum CosmicTaskStatus {
  COSMIC_TASK_STATUS_UNSPECIFIED = 0;
  COSMIC_TASK_STATUS_PENDING = 1;
  COSMIC_TASK_STATUS_RUNNING = 2;
  COSMIC_TASK_STATUS_COMPLETED = 3;
  COSMIC_TASK_STATUS_FAILED = 4;
  COSMIC_TASK_STATUS_FRACTAL_EXPANSION = 5;
  COSMIC_TASK_STATUS_GEOMETRIC_VALIDATION = 6;
}

message CosmicContext {
  string task_id = 1;
  string user_input = 2;
  uint32 current_step = 3;
  uint32 total_steps = 4;
  uint32 fractal_level = 5;
  string golden_ratio_state = 6;
  repeated PromptResponse previous_responses = 7;
  map<string, google.protobuf.Struct> cosmic_metadata = 8;
}

message FractalRequirements {
  optional CosmicContext context = 1;
  uint32 recursion_depth = 2;
  double self_similarity_threshold = 3;
  bool golden_ratio_compliance = 4;
  double fractal_dimension_target = 5;
  bool mobius_continuity = 6;
  double fractal_coherence = 7;
  repeated string expansion_criteria = 8;
}

// LLM and Prompt Types
message PromptRequest {
  repeated PromptMessage messages = 1;
  string model = 2;
  optional PromptContext context = 3;
  optional LLMPromptConfig llm_config = 4;
}

message PromptResponse {
  bytes id = 1;
  string provider = 2;
  string model = 3;
  string prompt = 4;
  string response = 5;
  google.protobuf.Timestamp timestamp = 6;
  TokenUsage tokens_used = 7;
  optional double cost = 8;
  optional uint64 latency_ms = 9;
}

message PromptMessage {
  string role = 1;
  string content = 2;
}

message PromptContext {
  optional string session_id = 1;
  optional string user_id = 2;
  optional string thread_id = 3;
}

message TokenUsage {
  uint32 prompt = 1;
  uint32 completion = 2;
  uint32 total = 3;
}

message LLMPromptConfig {
  double temperature = 1;
  uint32 max_tokens = 2;
  double top_p = 3;
  repeated string stop_sequences = 4;
}

// LLM Provider Types
message LLMProvider {
  string name = 1;
  string base_url = 2;
  repeated string supported_models = 3;
  LlmModel provider_type = 4;
}

enum LlmModel {
  AkashChat = 0;
  OllamaLocal = 1;
  KimiResearch = 2;
  Grok = 3;
  OpenAI = 4;
  Anthropic = 5;
  Custom = 6;
}

 // Local config structure that matches the existing implementation
message LocalLlmConfig {
  uint64 timeout_seconds = 1;
  string api_keys_file = 2;
}
// Configuration Types  
message HoConfig {
  cw_ho.network.v1.NetworkConfig network = 1;
  cw_ho.network.v1.NodeIdentity identity = 2;
  StorageConfig storage = 3;
  LlmRouterConfig llm = 4;
}

message StorageConfig {
  string data_dir = 1;
  uint32 max_size_mb = 2;
  bool enable_compression = 3;
}

message OpenAIRequest {
  string model = 1;
  repeated OpenAiMessage messages = 2;
  optional uint32 temperature = 3;
  optional uint32 max_tokens = 4;
}

message OpenAiUsage {
  uint32 prompt_tokens = 1;
  uint32 completion_tokens = 2;
  uint32 total_tokens = 3;
}
 
message OpenAiMessage {
  string role = 1;
  string content = 2;
}

message OpenAiResponse {
  repeated OpenAiChoice choices = 1;
  OpenAiUsage usage  = 2;
}


message OpenAiChoice {
  OpenAiMessage message  = 1;
}
 

enum ModelSelectionStrategy {
  MODEL_SELECTION_STRATEGY_UNSPECIFIED = 0;
  // Always use highest priority available
  MODEL_SELECTION_STRATEGY_PRIORITY = 1;
  // Round-robin across available providers
  MODEL_SELECTION_STRATEGY_ROUND_ROBIN = 2;
  // Golden ratio weighted selection
  MODEL_SELECTION_STRATEGY_GOLDEN_RATIO = 3;
  // Load-based selection
  MODEL_SELECTION_STRATEGY_LOAD_BALANCED = 4;
}
 
// Llm config is the global configuration of all llm models available, and their subconfigurations.
message LlmRouterConfig {
  string api_keys_file = 1;
  repeated LlmEntity entities = 2;
  ModelSelectionStrategy default_strategy = 3;
  uint64 timeout_seconds = 4;
  uint32 max_retries = 5;
  uint32 default_entity = 6;
}
 
/// LlmEntity is a single llm model entity. Contains information about available models, stragegy in use of the framework, and other configuration files 
message LlmEntity {
  string name = 1;
  string base_url = 2;
  repeated string models = 3;
  string default_model = 4;
  uint32 priority = 5; // Using uint32 for u8
  bool enabled = 6;
    ModelSelectionStrategy default_strategy = 7;
  uint64 timeout_seconds = 8;
  uint32 max_retries = 9;
}

message LoggingConfig {
  string level = 1;
  optional string file = 2;
}


 